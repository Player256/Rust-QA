{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"torch==2.2.2\" tensorboard --quiet\n",
    "# %pip install  --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\"  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import json, boto3, sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_bucket_name = \"applied-agi\"\n",
    "\n",
    "# s3_prefix = \"wasmedge/llama3-8b-instruct-262k-gguf\"\n",
    "# model_id = \"second-state/Llama-3-8B-Instruct-262k-GGUF\"\n",
    "# base_job_name = \"fsdp-llama3-8b-instruct-262k-gguf\"\n",
    "\n",
    "s3_prefix = \"wasmedge/llama3-8b-instruct-262k\"\n",
    "model_id = \"gradientai/Llama-3-8B-Instruct-262k\"\n",
    "base_job_name = \"fsdp-llama3-8b-instruct-262k\"\n",
    "\n",
    "train_dataset_file_path = \"/home/ubuntu/random-stuff/ashish/wasmedge/llm-coding-assistant/dataset/train_dataset_processed_processed.json\"\n",
    "validation_dataset_file_path = \"/home/ubuntu/random-stuff/ashish/wasmedge/llm-coding-assistant/dataset/validation_dataset_processed_processed.json\"\n",
    "test_dataset_file_path = \"/home/ubuntu/random-stuff/ashish/wasmedge/llm-coding-assistant/dataset/test_dataset_processed_processed.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role: arn:aws:iam::324622400514:role/ec2-vscode-role region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "session = sagemaker.session.Session(default_bucket=workspace_bucket_name)  # sagemaker session for interacting with different AWS APIs\n",
    "region = session._region_name  # region name of the current SageMaker Studio environment\n",
    "\n",
    "print(f'role: {role} region: {region}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/fsspec/registry.py:273: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b392fbc2714479cac275c7bf0a41cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851400a9ec2e48df9a47b4a7fbde6f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0968ef281924186890dfec23ce2ceb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3_train_dataset_path: s3://applied-agi/wasmedge/llama3-8b-instruct-262k/train\n",
      "s3_validation_dataset_path: s3://applied-agi/wasmedge/llama3-8b-instruct-262k/validation\n",
      "s3_test_dataset_path: s3://applied-agi/wasmedge/llama3-8b-instruct-262k/test\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=train_dataset_file_path,\n",
    "    split=\"train\"\n",
    ")\n",
    "s3_train_dataset_path = f's3://{workspace_bucket_name}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(s3_train_dataset_path)\n",
    "\n",
    "\n",
    "validation_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=validation_dataset_file_path,\n",
    "    split=\"train\"\n",
    ")\n",
    "s3_validation_dataset_path = f's3://{workspace_bucket_name}/{s3_prefix}/validation'\n",
    "validation_dataset.save_to_disk(s3_validation_dataset_path)\n",
    "\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=test_dataset_file_path,\n",
    "    split=\"train\"\n",
    ")\n",
    "s3_test_dataset_path = f's3://{workspace_bucket_name}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(s3_test_dataset_path)\n",
    "\n",
    "\n",
    "print(f\"s3_train_dataset_path: {s3_train_dataset_path}\")\n",
    "print(f\"s3_validation_dataset_path: {s3_validation_dataset_path}\")\n",
    "print(f\"s3_test_dataset_path: {s3_test_dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_train_dataset_path = session.upload_data(\n",
    "#     path=train_dataset_file_path, \n",
    "#     key_prefix=f\"{s3_prefix}/train\"\n",
    "# )\n",
    "# s3_validation_dataset_path = session.upload_data(\n",
    "#     path=validation_dataset_file_path, \n",
    "#     key_prefix=f\"{s3_prefix}/validation\"\n",
    "# )\n",
    "# s3_test_dataset_path = session.upload_data(\n",
    "#     path=test_dataset_file_path, \n",
    "#     key_prefix=f\"{s3_prefix}/test\"\n",
    "# )\n",
    "\n",
    "# print(f\"s3_train_dataset_path: {s3_train_dataset_path}\")\n",
    "# print(f\"s3_validation_dataset_path: {s3_validation_dataset_path}\")\n",
    "# print(f\"s3_test_dataset_path: {s3_test_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_job_name: fsdp-llama3-8b-instruct-262k\n",
      "checkpoint_s3_path: s3://applied-agi/wasmedge/llama3-8b-instruct-262k/checkpoints\n",
      "save_model_s3_path: s3://applied-agi/wasmedge/llama3-8b-instruct-262k/model/\n"
     ]
    }
   ],
   "source": [
    "print(f\"base_job_name: {base_job_name}\")\n",
    "\n",
    "checkpoint_dir = \"/opt/ml/checkpoints\"\n",
    "checkpoint_s3_path = f\"s3://{workspace_bucket_name}/{s3_prefix}/checkpoints\"\n",
    "print(f\"checkpoint_s3_path: {checkpoint_s3_path}\")\n",
    "\n",
    "save_model_s3_path = f\"s3://{workspace_bucket_name}/{s3_prefix}/model/\" # s3 path where model artifacts gets stored (Used when trying to save using s5cmd)\n",
    "print(f\"save_model_s3_path: {save_model_s3_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: \n",
      " {\n",
      "  \"model_id\": \"gradientai/Llama-3-8B-Instruct-262k\",\n",
      "  \"max_seq_len\": 1024,\n",
      "  \"s3_train_dataset_path\": \"s3://applied-agi/wasmedge/llama3-8b-instruct-262k/train\",\n",
      "  \"s3_test_dataset_path\": \"s3://applied-agi/wasmedge/llama3-8b-instruct-262k/test\",\n",
      "  \"sm_train_dataset_path\": \"/opt/ml/input/data/train\",\n",
      "  \"sm_test_dataset_path\": \"/opt/ml/input/data/test\",\n",
      "  \"_n_gpu\": 4,\n",
      "  \"output_dir\": \".\",\n",
      "  \"report_to\": \"tensorboard\",\n",
      "  \"learning_rate\": 0.0002,\n",
      "  \"lr_scheduler_type\": \"constant\",\n",
      "  \"num_train_epochs\": 3,\n",
      "  \"per_device_train_batch_size\": 1,\n",
      "  \"per_device_eval_batch_size\": 1,\n",
      "  \"gradient_accumulation_steps\": 1,\n",
      "  \"optim\": \"adamw_torch\",\n",
      "  \"logging_steps\": 10,\n",
      "  \"save_strategy\": \"epoch\",\n",
      "  \"evaluation_strategy\": \"epoch\",\n",
      "  \"max_grad_norm\": 0.3,\n",
      "  \"warmup_ratio\": 0.03,\n",
      "  \"bf16\": false,\n",
      "  \"tf32\": true,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"fsdp\": \"full_shard auto_wrap offload\",\n",
      "  \"save_model_s3_path\": \"s3://applied-agi/wasmedge/llama3-8b-instruct-262k/model/\",\n",
      "  \"checkpoint_dir\": \"/opt/ml/checkpoints\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# define hyperparameters\n",
    "hyperparameters = {\n",
    "    # script parameters\n",
    "    'model_id': model_id,\n",
    "    # \"max_seq_len\": 3072,\n",
    "    \"max_seq_len\": 1024,\n",
    "    's3_train_dataset_path': s3_train_dataset_path,\n",
    "    's3_test_dataset_path' : s3_test_dataset_path,\n",
    "    'sm_train_dataset_path': \"/opt/ml/input/data/train\",\n",
    "    'sm_test_dataset_path' : \"/opt/ml/input/data/test\",\n",
    "    \n",
    "    # training parameters\n",
    "    \"_n_gpu\": 4,\n",
    "    \"output_dir\": \".\",\n",
    "    \"report_to\": \"tensorboard\",               # report metrics to tensorboard\n",
    "    \"learning_rate\": 0.0002,                  # learning rate 2e-4\n",
    "    \"lr_scheduler_type\": \"constant\",          # learning rate scheduler\n",
    "    \"num_train_epochs\": 3,                    # number of training epochs\n",
    "    \"per_device_train_batch_size\": 1,         # batch size per device during training\n",
    "    \"per_device_eval_batch_size\": 1,          # batch size for evaluation\n",
    "    # \"gradient_accumulation_steps\": 2,         # number of steps before performing a backward/update pass\n",
    "    \"gradient_accumulation_steps\": 1,         # number of steps before performing a backward/update pass\n",
    "    \"optim\": \"adamw_torch\",                   # use torch adamw optimizer\n",
    "    \"logging_steps\": 10,                      # log every 10 steps\n",
    "    \"save_strategy\": \"epoch\",                 # save checkpoint every epoch\n",
    "    \"evaluation_strategy\": \"epoch\",           # evaluate every epoch\n",
    "    \"max_grad_norm\": 0.3,                     # max gradient norm\n",
    "    \"warmup_ratio\": 0.03,                     # warmup ratio\n",
    "    \"bf16\": False,                             # use bfloat16 precision\n",
    "    \"tf32\": True,                             # use tf32 precision\n",
    "    \"gradient_checkpointing\": True,           # use gradient checkpointing to save memory\n",
    "    \n",
    "    # FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "    \"fsdp\": \"full_shard auto_wrap offload\", # remove offload if enough GPU memory\n",
    "    # \"fsdp_config\": {\n",
    "    #     \"backward_prefetch\": \"backward_pre\",\n",
    "    #     \"forward_prefetch\": \"false\",\n",
    "    #     \"use_orig_params\": \"false\",\n",
    "    # },\n",
    "    \n",
    "    'save_model_s3_path': save_model_s3_path,\n",
    "    'checkpoint_dir': \"/opt/ml/checkpoints\",\n",
    "    }\n",
    "\n",
    "print('Hyperparameters: \\n', json.dumps(hyperparameters, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-27 19:41:31 Starting - Starting the training job...\n",
      "2024-05-27 19:41:45 Downloading - Downloading the training image\n",
      "2024-05-27 19:41:45 Training - Training image download completed. Training in progress.bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2024-05-27 19:41:46,414 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2024-05-27 19:41:46,451 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-27 19:41:46,463 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2024-05-27 19:41:46,465 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\n",
      "2024-05-27 19:41:46,465 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2024-05-27 19:41:47,804 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.10 -m pip install -r requirements.txt\n",
      "Collecting torch==2.2.2 (from -r requirements.txt (line 1))\n",
      "Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting transformers==4.40.0 (from -r requirements.txt (line 2))\n",
      "Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.6/137.6 kB 17.2 MB/s eta 0:00:00\n",
      "Collecting datasets==2.18.0 (from -r requirements.txt (line 3))\n",
      "Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate==0.29.3 (from -r requirements.txt (line 4))\n",
      "Downloading accelerate-0.29.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting evaluate==0.4.1 (from -r requirements.txt (line 5))\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting bitsandbytes==0.43.1 (from -r requirements.txt (line 6))\n",
      "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting huggingface_hub==0.22.2 (from -r requirements.txt (line 7))\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting trl==0.8.6 (from -r requirements.txt (line 8))\n",
      "Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft==0.10.0 (from -r requirements.txt (line 9))\n",
      "Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting s3fs>=2023.3.0 (from -r requirements.txt (line 10))\n",
      "Downloading s3fs-2024.5.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.2.2->-r requirements.txt (line 1)) (2024.5.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 1))\n",
      "Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 1))\n",
      "Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 1))\n",
      "Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2->-r requirements.txt (line 1))\n",
      "Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2->-r requirements.txt (line 1))\n",
      "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2->-r requirements.txt (line 1))\n",
      "Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2->-r requirements.txt (line 1))\n",
      "Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2->-r requirements.txt (line 1))\n",
      "Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2->-r requirements.txt (line 1))\n",
      "Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2->-r requirements.txt (line 1))\n",
      "Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 1))\n",
      "Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.2.0 (from torch==2.2.2->-r requirements.txt (line 1))\n",
      "Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 2)) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.40.0->-r requirements.txt (line 2))\n",
      "Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.0->-r requirements.txt (line 2))\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.40.0->-r requirements.txt (line 2))\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (16.1.0)\n",
      "Collecting pyarrow-hotfix (from datasets==2.18.0->-r requirements.txt (line 3))\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (2.2.2)\n",
      "Collecting xxhash (from datasets==2.18.0->-r requirements.txt (line 3))\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 3)) (0.70.16)\n",
      "Collecting fsspec (from torch==2.2.2->-r requirements.txt (line 1))\n",
      "Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets==2.18.0->-r requirements.txt (line 3))\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.3->-r requirements.txt (line 4)) (5.9.8)\n",
      "Collecting responses<0.19 (from evaluate==0.4.1->-r requirements.txt (line 5))\n",
      "Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tyro>=0.5.11 (from trl==0.8.6->-r requirements.txt (line 8))\n",
      "Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->-r requirements.txt (line 1))\n",
      "Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs>=2023.3.0->-r requirements.txt (line 10))\n",
      "Downloading aiobotocore-2.13.0-py3-none-any.whl.metadata (21 kB)\n",
      "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs>=2023.3.0 (from -r requirements.txt (line 10))\n",
      "Downloading s3fs-2024.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading s3fs-2024.3.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading s3fs-2024.2.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting botocore<1.34.107,>=1.34.70 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2023.3.0->-r requirements.txt (line 10))\n",
      "Downloading botocore-1.34.106-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting wrapt<2.0.0,>=1.10.10 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2023.3.0->-r requirements.txt (line 10))\n",
      "Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2023.3.0->-r requirements.txt (line 10))\n",
      "Downloading aioitertools-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3))\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3)) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3))\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3))\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3))\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets==2.18.0->-r requirements.txt (line 3))\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 2)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 8))\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 8)) (13.7.1)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 8))\n",
      "Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.2->-r requirements.txt (line 1)) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 3)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.2->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.34.107,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs>=2023.3.0->-r requirements.txt (line 10)) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 8)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 8)) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 8)) (0.1.2)\n",
      "Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 755.5/755.5 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.0/9.0 MB 94.6 MB/s eta 0:00:00\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.5/510.5 kB 49.7 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.6/297.6 kB 33.7 MB/s eta 0:00:00\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 13.8 MB/s eta 0:00:00\n",
      "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 17.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 38.8 MB/s eta 0:00:00\n",
      "Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.2/245.2 kB 27.8 MB/s eta 0:00:00\n",
      "Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.1/199.1 kB 27.2 MB/s eta 0:00:00\n",
      "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 4.0 MB/s eta 0:00:00\n",
      "Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 80.6 MB/s eta 0:00:00\n",
      "Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 62.5 MB/s eta 0:00:00\n",
      "Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 62.9 MB/s eta 0:00:00\n",
      "Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 19.6 MB/s eta 0:00:00\n",
      "Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 34.2 MB/s eta 0:00:00\n",
      "Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 18.1 MB/s eta 0:00:00\n",
      "Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 10.9 MB/s eta 0:00:00\n",
      "Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.0/166.0 MB 13.2 MB/s eta 0:00:00\n",
      "Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 14.7 MB/s eta 0:00:00\n",
      "Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.9/167.9 MB 14.4 MB/s eta 0:00:00\n",
      "Downloading s3fs-2024.2.0-py3-none-any.whl (28 kB)\n",
      "Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170.9/170.9 kB 30.0 MB/s eta 0:00:00\n",
      "Downloading aiobotocore-2.13.0-py3-none-any.whl (76 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.6/76.6 kB 14.3 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 79.9 MB/s eta 0:00:00\n",
      "Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 775.1/775.1 kB 64.5 MB/s eta 0:00:00\n",
      "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 77.6 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 96.9 MB/s eta 0:00:00\n",
      "Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.4/102.4 kB 19.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 29.5 MB/s eta 0:00:00\n",
      "Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading botocore-1.34.106-py3-none-any.whl (12.2 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.2/12.2 MB 91.9 MB/s eta 0:00:00\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 34.1 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 21.3 MB/s eta 0:00:00\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.3/80.3 kB 14.3 MB/s eta 0:00:00\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 38.5 MB/s eta 0:00:00\n",
      "Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/21.3 MB 69.1 MB/s eta 0:00:00\n",
      "Installing collected packages: xxhash, wrapt, triton, shtab, safetensors, regex, pyarrow-hotfix, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multidict, fsspec, frozenlist, docstring-parser, async-timeout, aioitertools, yarl, responses, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface_hub, botocore, aiosignal, tyro, tokenizers, nvidia-cusolver-cu12, aiohttp, transformers, torch, aiobotocore, s3fs, datasets, bitsandbytes, accelerate, trl, peft, evaluate\n",
      "Attempting uninstall: triton\n",
      "Found existing installation: triton 2.1.0\n",
      "Uninstalling triton-2.1.0:\n",
      "Successfully uninstalled triton-2.1.0\n",
      "Attempting uninstall: fsspec\n",
      "Found existing installation: fsspec 2024.5.0\n",
      "Uninstalling fsspec-2024.5.0:\n",
      "Successfully uninstalled fsspec-2024.5.0\n",
      "Attempting uninstall: botocore\n",
      "Found existing installation: botocore 1.34.108\n",
      "Uninstalling botocore-1.34.108:\n",
      "Successfully uninstalled botocore-1.34.108\n",
      "Attempting uninstall: torch\n",
      "Found existing installation: torch 2.2.0\n",
      "Uninstalling torch-2.2.0:\n",
      "Successfully uninstalled torch-2.2.0\n",
      "Attempting uninstall: s3fs\n",
      "Found existing installation: s3fs 0.4.2\n",
      "Uninstalling s3fs-0.4.2:\n",
      "Successfully uninstalled s3fs-0.4.2\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.22.0\n",
      "Uninstalling accelerate-0.22.0:\n",
      "Successfully uninstalled accelerate-0.22.0\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.32.108 requires botocore==1.34.108, but you have botocore 1.34.106 which is incompatible.\n",
      "boto3 1.34.108 requires botocore<1.35.0,>=1.34.108, but you have botocore 1.34.106 which is incompatible.\n",
      "Successfully installed accelerate-0.29.3 aiobotocore-2.13.0 aiohttp-3.9.5 aioitertools-0.11.0 aiosignal-1.3.1 async-timeout-4.0.3 bitsandbytes-0.43.1 botocore-1.34.106 datasets-2.18.0 docstring-parser-0.16 evaluate-0.4.1 frozenlist-1.4.1 fsspec-2024.2.0 huggingface_hub-0.22.2 multidict-6.0.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 peft-0.10.0 pyarrow-hotfix-0.6 regex-2024.5.15 responses-0.18.0 s3fs-2024.2.0 safetensors-0.4.3 shtab-1.7.1 tokenizers-0.19.1 torch-2.2.2 transformers-4.40.0 triton-2.2.0 trl-0.8.6 tyro-0.8.4 wrapt-1.16.0 xxhash-3.4.1 yarl-1.9.4\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "2024-05-27 19:43:26,345 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2024-05-27 19:43:26,345 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2024-05-27 19:43:26,407 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-27 19:43:26,456 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-27 19:43:26,469 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\n",
      "2024-05-27 19:43:26,505 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2024-05-27 19:43:26,518 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"_n_gpu\": 4,\n",
      "        \"bf16\": false,\n",
      "        \"checkpoint_dir\": \"/opt/ml/checkpoints\",\n",
      "        \"evaluation_strategy\": \"epoch\",\n",
      "        \"fsdp\": \"full_shard auto_wrap offload\",\n",
      "        \"gradient_accumulation_steps\": 1,\n",
      "        \"gradient_checkpointing\": true,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"logging_steps\": 10,\n",
      "        \"lr_scheduler_type\": \"constant\",\n",
      "        \"max_grad_norm\": 0.3,\n",
      "        \"max_seq_len\": 1024,\n",
      "        \"model_id\": \"gradientai/Llama-3-8B-Instruct-262k\",\n",
      "        \"num_train_epochs\": 3,\n",
      "        \"optim\": \"adamw_torch\",\n",
      "        \"output_dir\": \".\",\n",
      "        \"per_device_eval_batch_size\": 1,\n",
      "        \"per_device_train_batch_size\": 1,\n",
      "        \"report_to\": \"tensorboard\",\n",
      "        \"s3_test_dataset_path\": \"s3://applied-agi/wasmedge/llama3-8b-instruct-262k/test\",\n",
      "        \"s3_train_dataset_path\": \"s3://applied-agi/wasmedge/llama3-8b-instruct-262k/train\",\n",
      "        \"save_model_s3_path\": \"s3://applied-agi/wasmedge/llama3-8b-instruct-262k/model/\",\n",
      "        \"save_strategy\": \"epoch\",\n",
      "        \"sm_test_dataset_path\": \"/opt/ml/input/data/test\",\n",
      "        \"sm_train_dataset_path\": \"/opt/ml/input/data/train\",\n",
      "        \"tf32\": true,\n",
      "        \"warmup_ratio\": 0.03\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"fsdp-llama3-8b-instruct-262k-2024-05-27-19-41-30-964\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-324622400514/fsdp-llama3-8b-instruct-262k-2024-05-27-19-41-30-964/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_fsdp_trl\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_fsdp_trl.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"_n_gpu\":4,\"bf16\":false,\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"evaluation_strategy\":\"epoch\",\"fsdp\":\"full_shard auto_wrap offload\",\"gradient_accumulation_steps\":1,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_len\":1024,\"model_id\":\"gradientai/Llama-3-8B-Instruct-262k\",\"num_train_epochs\":3,\"optim\":\"adamw_torch\",\"output_dir\":\".\",\"per_device_eval_batch_size\":1,\"per_device_train_batch_size\":1,\"report_to\":\"tensorboard\",\"s3_test_dataset_path\":\"s3://applied-agi/wasmedge/llama3-8b-instruct-262k/test\",\"s3_train_dataset_path\":\"s3://applied-agi/wasmedge/llama3-8b-instruct-262k/train\",\"save_model_s3_path\":\"s3://applied-agi/wasmedge/llama3-8b-instruct-262k/model/\",\"save_strategy\":\"epoch\",\"sm_test_dataset_path\":\"/opt/ml/input/data/test\",\"sm_train_dataset_path\":\"/opt/ml/input/data/train\",\"tf32\":true,\"warmup_ratio\":0.03}\n",
      "SM_USER_ENTRY_POINT=run_fsdp_trl.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.12xlarge\",\"sagemaker_torch_distributed_enabled\":true}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"test\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=run_fsdp_trl\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=48\n",
      "SM_NUM_GPUS=4\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-324622400514/fsdp-llama3-8b-instruct-262k-2024-05-27-19-41-30-964/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.12xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"_n_gpu\":4,\"bf16\":false,\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"evaluation_strategy\":\"epoch\",\"fsdp\":\"full_shard auto_wrap offload\",\"gradient_accumulation_steps\":1,\"gradient_checkpointing\":true,\"learning_rate\":0.0002,\"logging_steps\":10,\"lr_scheduler_type\":\"constant\",\"max_grad_norm\":0.3,\"max_seq_len\":1024,\"model_id\":\"gradientai/Llama-3-8B-Instruct-262k\",\"num_train_epochs\":3,\"optim\":\"adamw_torch\",\"output_dir\":\".\",\"per_device_eval_batch_size\":1,\"per_device_train_batch_size\":1,\"report_to\":\"tensorboard\",\"s3_test_dataset_path\":\"s3://applied-agi/wasmedge/llama3-8b-instruct-262k/test\",\"s3_train_dataset_path\":\"s3://applied-agi/wasmedge/llama3-8b-instruct-262k/train\",\"save_model_s3_path\":\"s3://applied-agi/wasmedge/llama3-8b-instruct-262k/model/\",\"save_strategy\":\"epoch\",\"sm_test_dataset_path\":\"/opt/ml/input/data/test\",\"sm_train_dataset_path\":\"/opt/ml/input/data/train\",\"tf32\":true,\"warmup_ratio\":0.03},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"fsdp-llama3-8b-instruct-262k-2024-05-27-19-41-30-964\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-324622400514/fsdp-llama3-8b-instruct-262k-2024-05-27-19-41-30-964/source/sourcedir.tar.gz\",\"module_name\":\"run_fsdp_trl\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_fsdp_trl.py\"}\n",
      "SM_USER_ARGS=[\"--_n_gpu\",\"4\",\"--bf16\",\"False\",\"--checkpoint_dir\",\"/opt/ml/checkpoints\",\"--evaluation_strategy\",\"epoch\",\"--fsdp\",\"full_shard auto_wrap offload\",\"--gradient_accumulation_steps\",\"1\",\"--gradient_checkpointing\",\"True\",\"--learning_rate\",\"0.0002\",\"--logging_steps\",\"10\",\"--lr_scheduler_type\",\"constant\",\"--max_grad_norm\",\"0.3\",\"--max_seq_len\",\"1024\",\"--model_id\",\"gradientai/Llama-3-8B-Instruct-262k\",\"--num_train_epochs\",\"3\",\"--optim\",\"adamw_torch\",\"--output_dir\",\".\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"1\",\"--report_to\",\"tensorboard\",\"--s3_test_dataset_path\",\"s3://applied-agi/wasmedge/llama3-8b-instruct-262k/test\",\"--s3_train_dataset_path\",\"s3://applied-agi/wasmedge/llama3-8b-instruct-262k/train\",\"--save_model_s3_path\",\"s3://applied-agi/wasmedge/llama3-8b-instruct-262k/model/\",\"--save_strategy\",\"epoch\",\"--sm_test_dataset_path\",\"/opt/ml/input/data/test\",\"--sm_train_dataset_path\",\"/opt/ml/input/data/train\",\"--tf32\",\"True\",\"--warmup_ratio\",\"0.03\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP__N_GPU=4\n",
      "SM_HP_BF16=false\n",
      "SM_HP_CHECKPOINT_DIR=/opt/ml/checkpoints\n",
      "SM_HP_EVALUATION_STRATEGY=epoch\n",
      "SM_HP_FSDP=full_shard auto_wrap offload\n",
      "SM_HP_GRADIENT_ACCUMULATION_STEPS=1\n",
      "SM_HP_GRADIENT_CHECKPOINTING=true\n",
      "SM_HP_LEARNING_RATE=0.0002\n",
      "SM_HP_LOGGING_STEPS=10\n",
      "SM_HP_LR_SCHEDULER_TYPE=constant\n",
      "SM_HP_MAX_GRAD_NORM=0.3\n",
      "SM_HP_MAX_SEQ_LEN=1024\n",
      "SM_HP_MODEL_ID=gradientai/Llama-3-8B-Instruct-262k\n",
      "SM_HP_NUM_TRAIN_EPOCHS=3\n",
      "SM_HP_OPTIM=adamw_torch\n",
      "SM_HP_OUTPUT_DIR=.\n",
      "SM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\n",
      "SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\n",
      "SM_HP_REPORT_TO=tensorboard\n",
      "SM_HP_S3_TEST_DATASET_PATH=s3://applied-agi/wasmedge/llama3-8b-instruct-262k/test\n",
      "SM_HP_S3_TRAIN_DATASET_PATH=s3://applied-agi/wasmedge/llama3-8b-instruct-262k/train\n",
      "SM_HP_SAVE_MODEL_S3_PATH=s3://applied-agi/wasmedge/llama3-8b-instruct-262k/model/\n",
      "SM_HP_SAVE_STRATEGY=epoch\n",
      "SM_HP_SM_TEST_DATASET_PATH=/opt/ml/input/data/test\n",
      "SM_HP_SM_TRAIN_DATASET_PATH=/opt/ml/input/data/train\n",
      "SM_HP_TF32=true\n",
      "SM_HP_WARMUP_RATIO=0.03\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "Invoking script with the following command:\n",
      "torchrun --nnodes 1 --nproc_per_node 4 run_fsdp_trl.py --_n_gpu 4 --bf16 False --checkpoint_dir /opt/ml/checkpoints --evaluation_strategy epoch --fsdp full_shard auto_wrap offload --gradient_accumulation_steps 1 --gradient_checkpointing True --learning_rate 0.0002 --logging_steps 10 --lr_scheduler_type constant --max_grad_norm 0.3 --max_seq_len 1024 --model_id gradientai/Llama-3-8B-Instruct-262k --num_train_epochs 3 --optim adamw_torch --output_dir . --per_device_eval_batch_size 1 --per_device_train_batch_size 1 --report_to tensorboard --s3_test_dataset_path s3://applied-agi/wasmedge/llama3-8b-instruct-262k/test --s3_train_dataset_path s3://applied-agi/wasmedge/llama3-8b-instruct-262k/train --save_model_s3_path s3://applied-agi/wasmedge/llama3-8b-instruct-262k/model/ --save_strategy epoch --sm_test_dataset_path /opt/ml/input/data/test --sm_train_dataset_path /opt/ml/input/data/train --tf32 True --warmup_ratio 0.03\n",
      "[2024-05-27 19:43:27,743] torch.distributed.run: [WARNING] \n",
      "[2024-05-27 19:43:27,743] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-05-27 19:43:27,743] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-05-27 19:43:27,743] torch.distributed.run: [WARNING] *****************************************\n",
      "Detected extra arguments that are going to be ignored: ['--_n_gpu', '4', '--checkpoint_dir', '/opt/ml/checkpoints', 'auto_wrap', 'offload', '--save_model_s3_path', 's3://applied-agi/wasmedge/llama3-8b-instruct-262k/model/'] - make sure to double check what you are doing\n",
      "INFO: script_args: ScriptArguments(s3_train_dataset_path='s3://applied-agi/wasmedge/llama3-8b-instruct-262k/train', s3_test_dataset_path='s3://applied-agi/wasmedge/llama3-8b-instruct-262k/test', sm_train_dataset_path='/opt/ml/input/data/train', sm_test_dataset_path='/opt/ml/input/data/test', model_id='gradientai/Llama-3-8B-Instruct-262k', max_seq_length=1024)\n",
      "INFO: training_args: TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./runs/May27_19-43-37_algo-1,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=constant,\n",
      "max_grad_norm=0.3,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=.,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=.,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Detected extra arguments that are going to be ignored: ['--_n_gpu', '4', '--checkpoint_dir', '/opt/ml/checkpoints', 'auto_wrap', 'offload', '--save_model_s3_path', 's3://applied-agi/wasmedge/llama3-8b-instruct-262k/model/'] - make sure to double check what you are doing\n",
      "INFO: script_args: ScriptArguments(s3_train_dataset_path='s3://applied-agi/wasmedge/llama3-8b-instruct-262k/train', s3_test_dataset_path='s3://applied-agi/wasmedge/llama3-8b-instruct-262k/test', sm_train_dataset_path='/opt/ml/input/data/train', sm_test_dataset_path='/opt/ml/input/data/test', model_id='gradientai/Llama-3-8B-Instruct-262k', max_seq_length=1024)\n",
      "Detected extra arguments that are going to be ignored: ['--_n_gpu', '4', '--checkpoint_dir', '/opt/ml/checkpoints', 'auto_wrap', 'offload', '--save_model_s3_path', 's3://applied-agi/wasmedge/llama3-8b-instruct-262k/model/'] - make sure to double check what you are doing\n",
      "INFO: script_args: ScriptArguments(s3_train_dataset_path='s3://applied-agi/wasmedge/llama3-8b-instruct-262k/train', s3_test_dataset_path='s3://applied-agi/wasmedge/llama3-8b-instruct-262k/test', sm_train_dataset_path='/opt/ml/input/data/train', sm_test_dataset_path='/opt/ml/input/data/test', model_id='gradientai/Llama-3-8B-Instruct-262k', max_seq_length=1024)\n",
      "INFO: training_args: TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./runs/May27_19-43-37_algo-1,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=constant,\n",
      "max_grad_norm=0.3,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=.,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=.,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "INFO: training_args: TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=2,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./runs/May27_19-43-37_algo-1,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=constant,\n",
      "max_grad_norm=0.3,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=.,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=.,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Detected extra arguments that are going to be ignored: ['--_n_gpu', '4', '--checkpoint_dir', '/opt/ml/checkpoints', 'auto_wrap', 'offload', '--save_model_s3_path', 's3://applied-agi/wasmedge/llama3-8b-instruct-262k/model/'] - make sure to double check what you are doing\n",
      "INFO: script_args: ScriptArguments(s3_train_dataset_path='s3://applied-agi/wasmedge/llama3-8b-instruct-262k/train', s3_test_dataset_path='s3://applied-agi/wasmedge/llama3-8b-instruct-262k/test', sm_train_dataset_path='/opt/ml/input/data/train', sm_test_dataset_path='/opt/ml/input/data/test', model_id='gradientai/Llama-3-8B-Instruct-262k', max_seq_length=1024)\n",
      "INFO: training_args: TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=3,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./runs/May27_19-43-37_algo-1,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=constant,\n",
      "max_grad_norm=0.3,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=.,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=.,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   0%|          | 0/854 [00:00<?, ? examples/s]\n",
      "Map:   0%|          | 0/854 [00:00<?, ? examples/s]\n",
      "Map:   0%|          | 0/854 [00:00<?, ? examples/s]\n",
      "Map:   0%|          | 0/854 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 854/854 [00:00<00:00, 6089.11 examples/s]\n",
      "Map: 100%|██████████| 854/854 [00:00<00:00, 6036.23 examples/s]\n",
      "Map: 100%|██████████| 854/854 [00:00<00:00, 5871.79 examples/s]\n",
      "Map: 100%|██████████| 854/854 [00:00<00:00, 5817.32 examples/s]\n",
      "Map: 100%|██████████| 854/854 [00:00<00:00, 5874.25 examples/s]\n",
      "Map: 100%|██████████| 854/854 [00:00<00:00, 5824.81 examples/s]\n",
      "Map: 100%|██████████| 854/854 [00:00<00:00, 5770.21 examples/s]\n",
      "Map: 100%|██████████| 854/854 [00:00<00:00, 5721.44 examples/s]\n",
      "Map:   0%|          | 0/107 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 107/107 [00:00<00:00, 6307.67 examples/s]\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Downloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.10s/it]\n",
      "Downloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.13s/it]\n",
      "Downloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.13s/it]\n",
      "Downloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.13s/it]\n",
      "Downloading shards:  50%|█████     | 2/4 [00:25<00:25, 12.79s/it]\n",
      "Downloading shards:  50%|█████     | 2/4 [00:25<00:25, 12.79s/it]\n",
      "Downloading shards:  50%|█████     | 2/4 [00:25<00:25, 12.79s/it]\n",
      "Downloading shards:  50%|█████     | 2/4 [00:25<00:25, 12.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 26\u001b[0m\n\u001b[1;32m      1\u001b[0m estimator \u001b[38;5;241m=\u001b[39m PyTorch(\n\u001b[1;32m      2\u001b[0m     base_job_name                \u001b[38;5;241m=\u001b[39m base_job_name,\n\u001b[1;32m      3\u001b[0m     source_dir                   \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./scripts\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     distribution\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_distributed\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}} \u001b[38;5;66;03m# enable torchrun\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: s3_train_dataset_path,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m : s3_test_dataset_path,\n\u001b[1;32m     24\u001b[0m }\n\u001b[0;32m---> 26\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# logs='None'\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/sagemaker/estimator.py:1346\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/sagemaker/estimator.py:2703\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2703\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2704\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2705\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/sagemaker/session.py:5797\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   5776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   5777\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   5778\u001b[0m \n\u001b[1;32m   5779\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5795\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   5796\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5797\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/sagemaker/session.py:7976\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   7973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m LogState\u001b[38;5;241m.\u001b[39mCOMPLETE:\n\u001b[1;32m   7974\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 7976\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m LogState\u001b[38;5;241m.\u001b[39mJOB_COMPLETE:\n\u001b[1;32m   7979\u001b[0m     state \u001b[38;5;241m=\u001b[39m LogState\u001b[38;5;241m.\u001b[39mCOMPLETE\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimator = PyTorch(\n",
    "    base_job_name                = base_job_name,\n",
    "    source_dir                   = \"./scripts\",\n",
    "    entry_point                  = \"run_fsdp_trl.py\",\n",
    "    role                         = role,\n",
    "    framework_version            = \"2.2.0\",\n",
    "    py_version                   = \"py310\", \n",
    "    instance_count               = 1,\n",
    "    instance_type                = \"ml.g5.12xlarge\", # 4\n",
    "    # instance_type                = \"ml.g5.48xlarge\", # 8\n",
    "    # instance_type                = \"ml.p4d.24xlarge\", # 8\n",
    "    hyperparameters              = hyperparameters,\n",
    "    checkpoint_local_path        = checkpoint_dir,   \n",
    "    checkpoint_s3_uri            = checkpoint_s3_path,\n",
    "    disable_profiler             = True,\n",
    "    keep_alive_period_in_seconds = 1800,\n",
    "    debugger_hook_config         = False,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}} # enable torchrun\n",
    ")\n",
    "\n",
    "data = {\n",
    "    'train': s3_train_dataset_path,\n",
    "    'test' : s3_test_dataset_path,\n",
    "}\n",
    "\n",
    "estimator.fit(data, wait=True) # logs='None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define hyperparameters\n",
    "# hyperparameters = {\n",
    "#     # script parameters\n",
    "#     'model_id': model_id,\n",
    "    \n",
    "#     'sm_train_dataset_path': \"/opt/ml/input/data/train\",\n",
    "#     'sm_test_dataset_path' : \"/opt/ml/input/data/test\",\n",
    "#     \"epochs\": 3,                    # number of training epochs\n",
    "#     \"per_device_train_batch_size\": 1,         # batch size per device during training\n",
    "#     \"optimizer\": \"adamw_torch\",                   # use torch adamw optimizer\n",
    "#     \"gradient_checkpointing\": True,           # use gradient checkpointing to save memory\n",
    "#     \"bf16\": True,                             # use bfloat16 precision\n",
    "#     \"fsdp\": \"full_shard auto_wrap offload\", # remove offload if enough GPU memory\n",
    "    \n",
    "    \n",
    "#     \"max_seq_len\": 3072,\n",
    "#     \"output_dir\": \".\",\n",
    "#     \"report_to\": \"tensorboard\",               # report metrics to tensorboard\n",
    "#     \"learning_rate\": 0.0002,                  # learning rate 2e-4\n",
    "#     \"lr_scheduler_type\": \"constant\",          # learning rate scheduler\n",
    "#     # \"per_device_eval_batch_size\": 1,          # batch size for evaluation\n",
    "#     \"gradient_accumulation_steps\": 2,         # number of steps before performing a backward/update pass\n",
    "#     \"logging_steps\": 10,                      # log every 10 steps\n",
    "#     \"save_strategy\": \"epoch\",                 # save checkpoint every epoch\n",
    "#     \"evaluation_strategy\": \"epoch\",           # evaluate every epoch\n",
    "#     \"max_grad_norm\": 0.3,                     # max gradient norm\n",
    "#     \"warmup_ratio\": 0.03,                     # warmup ratio\n",
    "#     \"tf32\": True,                             # use tf32 precision\n",
    "    \n",
    "    \n",
    "#     # FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "    \n",
    "#     \"fsdp_config\": {\n",
    "#         \"backward_prefetch\": \"backward_pre\",\n",
    "#         \"forward_prefetch\": \"false\",\n",
    "#         \"use_orig_params\": \"false\",\n",
    "#     },\n",
    "    \n",
    "#     'save_model_s3_path': save_model_s3_path,\n",
    "#     'checkpoint_dir': \"/opt/ml/checkpoints\",\n",
    "#     }\n",
    "\n",
    "# print('Hyperparameters: \\n', json.dumps(hyperparameters, indent=2, default=str))\n",
    "\n",
    "# estimator = PyTorch(\n",
    "#     base_job_name                = base_job_name,\n",
    "#     source_dir                   = \"./scripts\",\n",
    "#     entry_point                  = \"run_fsdp_trl.py\",\n",
    "#     role                         = role,\n",
    "#     framework_version            = \"2.2.0\",\n",
    "#     py_version                   = \"py310\", \n",
    "#     instance_count               = 1,\n",
    "#     instance_type                = \"ml.g5.12xlarge\",\n",
    "#     hyperparameters              = hyperparameters,\n",
    "#     checkpoint_local_path        = checkpoint_dir,   \n",
    "#     checkpoint_s3_uri            = checkpoint_s3_path,\n",
    "#     disable_profiler             = True,\n",
    "#     keep_alive_period_in_seconds = 1800,\n",
    "#     debugger_hook_config         = False,\n",
    "# )\n",
    "\n",
    "# data = {\n",
    "#     'train': s3_train_dataset_path,\n",
    "#     'test' : s3_test_dataset_path,\n",
    "# }\n",
    "\n",
    "# estimator.fit(data, wait=True) # logs='None'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
